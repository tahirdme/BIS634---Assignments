{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ea13a3",
   "metadata": {},
   "source": [
    "# BIS634 Asssignment 4 \n",
    "  \n",
    "## Question 1   \n",
    "\n",
    "### Tahir Manuel D'Mello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718131b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88945d53",
   "metadata": {},
   "source": [
    "Implement a two-dimensional version of the gradient descent algorithm to find optimal choices of a and b. **(7 points)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b383fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.294915"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(a, b):\n",
    "\n",
    "    query = f'http://ramcdougal.com/cgi-bin/error_function.py?a={a}&b={b}'\n",
    "    f = float(requests.get(query, headers={\"User-Agent\": \"MyScript\"}).text)\n",
    "    \n",
    "    return f\n",
    "\n",
    "func(0.4, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc3dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(a_initial, b_initial, stopping, h, gamma):\n",
    "    \n",
    "    a_old = a_initial\n",
    "    b_old = b_initial\n",
    "    f_old = func(a_old, b_old)\n",
    "    \n",
    "    fprime_a = ( func(a_old + h, b_old) - f_old ) / h\n",
    "    fprime_b = ( func(a_old, b_old + h) - f_old ) / h\n",
    "\n",
    "    a_next = a_old - gamma*fprime_a\n",
    "    b_next = b_old - gamma*fprime_b\n",
    "\n",
    "    f_next = func(a_next, b_next)\n",
    "    \n",
    "    #print(\"Function is\", f_next)\n",
    "    \n",
    "    while abs(f_old -  f_next) > stopping:\n",
    "        a_old = a_next\n",
    "        b_old = b_next\n",
    "        f_old = f_next\n",
    "\n",
    "        fprime_a = ( func(a_old + h, b_old) - f_old ) / h\n",
    "        fprime_b = ( func(a_old, b_old + h) - f_old ) / h\n",
    "\n",
    "        a_next = a_old - gamma*fprime_a\n",
    "        b_next = b_old - gamma*fprime_b\n",
    "        f_next = func(a_next, b_next)\n",
    "        \n",
    "        #print(\"Function is\", f_next)\n",
    "        \n",
    "    return a_next, b_next, f_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1e395",
   "metadata": {},
   "source": [
    "Explain how you estimate the gradient given that you cannot directly compute the derivative. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e21987",
   "metadata": {},
   "source": [
    "The gradients for both a and b were computed using the following mathematical formulae:  \n",
    "$\\displaystyle \\frac{\\partial f}{\\partial x} (a,b) = \\lim_{h\\to 0} \\frac{f(a+h,b) - f(a,b)}{h}$\n",
    "\n",
    "$\\displaystyle \\frac{\\partial f}{\\partial y} (a,b) = \\lim_{h\\to 0} \\frac{f(a,b+h) - f(a,b)}{h}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2459f",
   "metadata": {},
   "source": [
    "identify any numerical choices -- including but not limited to stopping criteria -- you made **(3 points)**, and justify why you think they were reasonable choices **(3 points)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ed23a",
   "metadata": {},
   "source": [
    "There were 3 numerical choices made:  \n",
    "1. Stopping criteria - Iterations were stopped when the difference between the minima picked in consecutive iterations didn't differ by more than a picked value (0.000001 in this case). This was a reasonable ned point to aim for.  \n",
    "2. Gradient displacement h - This was a small value chosen to compute the mathematical value of the gradient being calculated. Value chosen was 0.0001 as this ensured a reasonable estimate of the gradient.   \n",
    "3. Gamma - This determines the size of the step. An appropriate value of the stepsize (0.1) was chosen to ensure that the minima is reached without overtaking the point while still keeping the number of iterations needed down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451dd90e",
   "metadata": {},
   "source": [
    "Find both locations (i.e. a, b values) querying the API as needed **(5 points)** and identify which corresponds to which (local minimum and global minimum). **(2 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db87ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a is at 0.7117455600003538\n",
      "Optimal b is at 0.16897035000009702\n",
      "Minima of function is 1.00000019686\n"
     ]
    }
   ],
   "source": [
    "a, b, f = gradient_descent(0.4, 0.2, 0.000001, 0.0001, 0.1)\n",
    "print('Optimal a is at' , a)\n",
    "print('Optimal b is at' , b)\n",
    "print(\"Minima of function is\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07bc5f7",
   "metadata": {},
   "source": [
    "This is the global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07d2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a is at 0.2171203699999562\n",
      "Optimal b is at 0.689218500000343\n",
      "Minima of function is 1.10000130297\n"
     ]
    }
   ],
   "source": [
    "a, b, f = gradient_descent(0.7, 0.8, 0.000001, 0.0001, 0.1)\n",
    "print('Optimal a is at' , a)\n",
    "print('Optimal b is at' , b)\n",
    "print(\"Minima of function is\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88318117",
   "metadata": {},
   "source": [
    "This is the local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb52af6",
   "metadata": {},
   "source": [
    "Briefly discuss how you would have tested for local vs global minima if you had not known how many minima there were. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64869c",
   "metadata": {},
   "source": [
    "A fee methods to test for local vs gobal minima in the gradient decent algorithm:\n",
    "\n",
    "1. Vary initial guesses across the set of input values possible.\n",
    "2. Vary the learning rate (varying gamma in this case) for various input values to ensure that the step size doesn't overshoot any minima. \n",
    "3. Other techniques like stochastic gradient descent can be implemented (randomly without replacement selects one direction to perform gradient descent per iteration) which give us more directions to move in and can help escape local minima. \n",
    "\n",
    "Then at the end, the smallest value found is (probably, if done correctly) the glocal minima and the rest are all local minima. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
